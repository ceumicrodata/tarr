REQs:

- able to give source as parameter
- able to give description (e.g. "location type: addresses of managing board members (rovat_15)")
- able to separate jobs (identify them)

- able to process data in small batches:
	- able to process data in a batch
	- able to process data in all batches
	- able to process data in parallel
	- able to test the process by running a test on a small sample
- able to restart killed process without any data loss/side effect (transactions!), continuing where it was killed

- able to modify (fix) code and continue a job
- able to find out if a batch was completed with a modified code (relaxed: modified config file?)

- able to store away exceptional data for further processing (e.g. human input)
- able to process stored away data (use it as source)

- able to get information/statistics of a job (time_created, source, config, description, # of exceptions present, execution summary)
- able to get DAG statistics for a job (node timing, node success/failure numbers)
- able to list batch status of a job (processed/not processed, config, time_completed)
- able to delete all data for a job




DESIGN:


TABLE batch.partition:

	partitioning_name
	batch_id


TABLE batch.batch:
	batch_id
	ceg_id

# auxiliary tables to help splitting up complex's data into batches
#
# partitioning_name -> batch_ids
# batch_id -> ceg_id
#
# partitioning_name_name allows for multiple batch schemas/sizes to coexist
# example (predefined?) batch schemas: size_5000, size_10000, size_1000, every_200, test1, test2, ...
#
# every_200: every 200th record goes into same batch, should produce a random sample - the input is ordered by ceg_id, so should produce a deterministic batch partitioning

class Partition:
	partitioning_name
	batch_ids


TABLE tarr.job:

	job_id
	time_created
	application
	config
	config_hash
	partitioning_name
	source ('complex:rovat_13:pc' for data in complex db table rovat_13 selecting fields starting with 'pc': pcirsz, pchely, pcteru, ...)
	params (json?)
	description
	state (ongoing/finished?)

# application: dotted name of class to handle the job
# config: it is not the full config, rather an identifier (name)
# the full config might be stored in another table
#
# params:
# to define data source? ('rovat_5', 'rovat_6', 'rovat_7')
# to give variations ('rovat_13' has 3 location variations with prefixes: '' 'p' and 'pc' ('rovat_14', 'rovat_15' has 4 by introducting another prefix 'pm'))
# to give data type (like X in: "location for X")



TABLE batch job state:

	batch_id
	job_id
	time_completed
	config_hash
	(stats in json?)

# over time the config file (and the program) can change
# that change might be harmful, so it is worth knowing if it happened



TABLE DagStatistic:

	dagstat_id

	node_name
	item_count
	success_count
	failure_count
	run_time


Application specific/defined:

TABLE exception tracking:

	job_id
	batch_id
	item_id
	exception_details # json?
	is_resolved # boolean

# items requiring human input



TABLE result:

	job_id
	item_id
	value



class Job:

	batch_ids
	items_for(batch_id) # normal batch processing
	item_for(item_id) # exceptional data



usage:

create_job(config, batch_name)
	creates Job(config, batch_name)
	creates Batches for the job

job = Job.by_id(job_id)
jobrunner = JobRunner(job)
jobrunner.process(batch_id) -> populates result, exception tracking and administers batch job state
jobrunner.process_exception() -> potential human input

summarize(job_id)


Job.list_all()
Job.delete(job_id)
Job.by_id(job_id).list()


Application:

	dag

	create_job
	create_batches
	process_job
	process_batch
	# process_exception() ?
	store_batch_statistics
	delete_job
